{
  "comments": [
    {
      "key": {
        "uuid": "f0820c18_90003c9f",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 343,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2014-10-09T01:50:16Z",
      "side": 1,
      "message": "I don\u0027t see how this can legitimately make a difference, unless there are interesting hardware coherence issues.  There is already a fence to ensure that the contents of the critical section can\u0027t move out of the CS.  The only interesting shared memory accesses getting ordered by the added fence seem to be to mutex-\u003evalue, and those are all to exactly the same location, and hence should not require fencing.\n\nHaving said that, I\u0027m very mildly worried about the algorithm used here.  The comments are a reasonable argument, but I didn\u0027t find them 100% convincing.  A more precise proof seems surprisingly difficult to come by.  I suspect it would be simpler if we had lock and unlock each basically perform load; CAS combination rather than going through multiple atomic steps.\n\nNonetheless, here\u0027s my attempt to show that this doesn\u0027t lose a wakeups:\n\nObservations:\n\nWhenever the state first enters 2, we know that either the futex_wait won\u0027t wait or some thread will eventually be woken.\nEither the thread currently holding the lock will do so, or if\nthe lock was not held, we will.\n\nWhenever the state decreases from 2, wake() is eventually called.\n\nAssume a thread blocks forever in the futex_wait without\nbeing woken.  Consider the last atomic decrement that resulted in a wake call, say by thread 1.  At the wake call we were in one of the following cases:\n\nThere were no waiters.  Means the state never became contended after that \u003d\u003d\u003e impossible; there can\u0027t be any waiters.\n\nstate was unlocked when woken thread called swap\u003d\u003d\u003e woken thread would have made progress, set the state to contended and wake() would have been called again.\n\nstate was locked \u003d\u003d\u003e thread setting state to locked would have made progress, since woken thread would reset state to contended.\n\nstate contended \u003d\u003d\u003e Thread 1 set the state to unlocked.  Since then, some thread 2 set state to locked, some thread 3 is the first to set it to contended after observing the locked state.  If thread 2 completes the unlock decrement before the state is set to contended, thread 3 will make another later decrement wake call, since it sees an unlocked mutex value but nontheless sets the state to contended.  Otherwise thread 2 will do so after seeing the contended state.\n\nIn all cases we get a contradiction.\n\nThus, in spite of my negative comments, I actually kind of believe the algorithm as is.",
      "range": {
        "startLine": 343,
        "startChar": 9,
        "endLine": 343,
        "endChar": 31
      },
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "50c2f81e_c51475b4",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 343,
      "author": {
        "id": 1037505
      },
      "writtenOn": "2014-10-09T17:05:49Z",
      "side": 1,
      "message": "Thanks Hans. I agree with you. I find it weird that a barrier could do any difference at that position, when the compare-exchange is guaranteed to have failed. After all, if the cmpxchg fails, then this means that the current thread hasn\u0027t written anything to memory. Why would it need a barrier, then?\n\nOrjan Eide, who has carried out most of the investigations on the issue so far, has just confirmed that the barrier above does only reduce the rate at which the bug occurs. In other words, the fix in this CL is know not to work. He has moved the barrier below (see comment) and this - so far - seems to have removed the issue (or has lowered the occurrence rate enough to require longer observation times). I hope I will have more time to look into this tomorrow (so far I have been hugely distracted by other issues and couldn\u0027t even attempt to reproduce the bug myself).",
      "parentUuid": "f0820c18_90003c9f",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "502d9828_35ae50f5",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 345,
      "author": {
        "id": 1037505
      },
      "writtenOn": "2014-10-13T20:46:31Z",
      "side": 1,
      "message": "LABEL1:",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "d087c827_afa95d9f",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 379,
      "author": {
        "id": 1037505
      },
      "writtenOn": "2014-10-09T17:05:49Z",
      "side": 1,
      "message": "A barrier here seems to work better.",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "508af82f_53dfbc1b",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 379,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2014-10-09T18:40:35Z",
      "side": 1,
      "message": "The failure is a deadlock on ARMv8?\n\nThis at least weakly suggests that the __futex_wake_ex call may not ensure ordering we expect, and the assignment to value is not yet visible when the other thread wakes up.  That would also break other code in Android.\n\nBased on a quick examination of the kernel code, it is conceivable to me that there may be a problem here that was introduced with ARMv8.  Futex_wake acquires a spin lock and then calls wake_futex.  The spin lock presumably used to include a dmb which would have somewhat accidentally prevented this.  That\u0027s presumably now an exclusive acquire load, which would not.  I would be mildly surprised if there is not another dmb on kernel entry to prevent this.  If this is indeed the problem, it should also break on Itanium, though possibly not on real hardware.\n\nIf this is the problem, I think the correct place to fix it is in the kernel.  The fence here may be an actual workaround, but we would need several more like it.",
      "parentUuid": "d087c827_afa95d9f",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b0ba54ec_58b67d60",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 379,
      "author": {
        "id": 1037505
      },
      "writtenOn": "2014-10-10T17:31:52Z",
      "side": 1,
      "message": "If I read the bug report correctly the issue has been observed when running both 32-bit and 64-bit versions of the same application on 64-bit Android (this should be confirmed shortly) on an ARMv8 system. It seems to only occurs when running threads across different CPU clusters. One thread hangs forever in __futex_wake_ex(). Orjan\u0027s gdb investigations suggest that the thread should have been waked up, but it hasn\u0027t. Note that the issue hasn\u0027t yet been observed when forcing the threads to run on the same cluster ([all on A53 cores] or [all on A57 cores]). While I didn\u0027t have a lot of time today to work on this, I had enough to prepare a filesystem and reproduce the issue. Hopefully, next week I\u0027ll gather more info.",
      "parentUuid": "508af82f_53dfbc1b",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "10f960d6_e61c8092",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 379,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2014-10-10T18:07:33Z",
      "side": 1,
      "message": "That still seems consistent with my conjecture that it\u0027s the lack of a fence in the kernel.  The deciding factor would be whether the kernel spin-lock implementation uses an acquire load or a dmb, which would presumably depend only on whether the kernel is 64-bit.  This would be highly dependent on weird timing, so it wouldn\u0027t surprise me if it occurred only in certain CPU configurations.  But the more I think about, the more I remain highly suspicious of my conjecture that there is no other kernel synchronization that would force sufficient memory ordering.\n\nDo you have a kernel expert involved or should I see if I can find one here?",
      "parentUuid": "b0ba54ec_58b67d60",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9042b0da_22229041",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 379,
      "author": {
        "id": 1037505
      },
      "writtenOn": "2014-10-12T09:46:06Z",
      "side": 1,
      "message": "An issue in the kernel would be a valid candidate for explaining the observations collected so far ...but I share the suspect. It sounds strange to me that the thread could go through a system call without any sort of memory synchronisation.\n\nTomorrow, I\u0027ll ask some of our kernel guys and see what they say. I\u0027ll let you know of any developments...\n\nMany thanks for helping with this :-)",
      "parentUuid": "10f960d6_e61c8092",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9042b0da_c5464e9b",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 401,
      "author": {
        "id": 1037505
      },
      "writtenOn": "2014-10-13T20:46:31Z",
      "side": 1,
      "message": "I think we found a potential weakness that could arise if the __futex_wake_ex() below is allowed to return zero (which - I believe - is the case).\n\nAssume a thread was interrupted - for any reasons - at \"LABEL1\". The __futex_wake_ex() below would have nothing to wake and would return 0. Once the previously-interrupted thread gets a chance to continue from \"LABEL1\" it goes into a __futex_wait_ex() and sleeps forever.\n\nTomorrow I will replace the line below with:\n\n  int num_woken_threads \u003d __futex_wake_ex(\u0026mutex-\u003evalue, shared, 1);\n  DCHECK_EQ(num_woken_threads, 1);\n\nTo check whether this is what we are observing. If I understand correctly, a failure in the assertion above can - in principle - cause one thread to sleep forever. If this is what is happening, we still would have to explain why running on different CPU clusters matters... I\u0027ll let you know how it goes.",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "d01348da_dd482b8c",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 401,
      "author": {
        "id": 1042828
      },
      "writtenOn": "2014-10-13T23:04:56Z",
      "side": 1,
      "message": "I don\u0027t think it\u0027s that simple, unfortunately.  If unlock was executed entirely while another thread was stopped at Label1 then mutex-\u003e value would no longer be locked_contended, and the __futex_wait_ex would have returned immediately, since the comparison would fail.  If another thread set it back to locked_contended in the meantime, then that thread, or its successor, would wake up the waiter.  Such is the theory, anyway.\n\nI think the proposed DCHECK would trigger in normal operation and not tell you much.  The last wake in a contention period will fail to wake up another thread, even during normal operation. \n\nThis is a surprisingly complicated and subtle algorithm for what it does.  In the long run, I think a simplification attempt (single CAS loop in unlock rather than 2 stage release, replace the slower path in lock() with something that treats unlocked the same as we would have in the fast path) may make sense.  But I\u0027m not convinced it\u0027s really broken.  And if the real problem is in __futex_wake_ex, then this is not the only broken functionality.",
      "parentUuid": "9042b0da_c5464e9b",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "d086e864_51421438",
        "filename": "libc/bionic/pthread_mutex.cpp",
        "patchSetId": 1
      },
      "lineNbr": 401,
      "author": {
        "id": 1037505
      },
      "writtenOn": "2014-10-14T21:33:26Z",
      "side": 1,
      "message": "I agree, it is not that simple. __futex_wake_ex() should be able to return zero.\n\nWe are currently looking for missing barriers in kernel/futex.c:__futex_wake_ex(). There are a number of cases for which this system call returns without going through a barrier. Also, this function call has been subject to some barrier-related fixes recently:\n\nb0c29f79ecea0b6fbcefc999e70f2843ae8306db (futexes: Avoid taking the hb-\u003elock if there\u0027s nothing to wake up)",
      "parentUuid": "d01348da_dd482b8c",
      "revId": "374a61d979eb82ac83a46dc61e9b50939f22d324",
      "serverId": "85c56323-6fa9-3386-8a01-6480fb634889",
      "unresolved": false
    }
  ]
}